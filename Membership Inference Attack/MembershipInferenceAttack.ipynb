{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class TargetModel(nn.Module):\n",
        "    \"\"\"\n",
        "        Target model for classification.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(TargetModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(30, 64)\n",
        "        self.fc2 = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return torch.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "hUYSj3fb_uCf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShadowModel(nn.Module):\n",
        "    \"\"\"\n",
        "        Shadow model for membership inference.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(ShadowModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(30, 64)\n",
        "        self.fc2 = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return torch.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "tzqK_IyU_2_h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def train_target_model(X_train, y_train, X_test, y_test, num_epochs):\n",
        "    \"\"\"\n",
        "        Trains the target model.\n",
        "\n",
        "        Args:\n",
        "            X_train (torch.Tensor): Training data.\n",
        "            y_train (torch.Tensor): Training labels.\n",
        "            X_test (torch.Tensor): Test data.\n",
        "            y_test (torch.Tensor): Test labels.\n",
        "\n",
        "        Returns:\n",
        "            TargetModel: Trained target model.\n",
        "    \"\"\"\n",
        "    target_model = TargetModel()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(target_model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = target_model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        target_model.eval()\n",
        "        outputs = target_model(X_test)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = accuracy_score(predicted.numpy(), y_test.numpy())\n",
        "        print(f\"Accuracy of the target model: {accuracy}\")\n",
        "\n",
        "    return target_model"
      ],
      "metadata": {
        "id": "co8QEPKl_8ig"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "        Generator network for creating synthetic data.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "22ZubQMADv80"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "        Discriminator network for discriminating\n",
        "        between real and synthetic data.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "mOKFMrq6Dx2j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan_single_data_point(data_point, generator, discriminator,\n",
        "                                num_epochs):\n",
        "    \"\"\"\n",
        "        Trains a GAN using just a single data point.\n",
        "\n",
        "        Args:\n",
        "            data_point (numpy.ndarray): Single data point to use for training.\n",
        "            generator (Generator): Generator network.\n",
        "            discriminator (Discriminator): Discriminator network.\n",
        "            num_epochs (int): Number of epochs for training.\n",
        "    \"\"\"\n",
        "    criterion = nn.BCELoss()\n",
        "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
        "    g_optimizer = optim.Adam(generator.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        discriminator.zero_grad()\n",
        "\n",
        "        real_data = torch.tensor(data_point).float().unsqueeze(0)\n",
        "        real_labels = torch.ones((1, 1))\n",
        "\n",
        "        real_output = discriminator(real_data)\n",
        "        d_real_loss = criterion(real_output, real_labels)\n",
        "\n",
        "        noise = torch.randn(1, 100)\n",
        "        fake_data = generator(noise)\n",
        "        fake_labels = torch.zeros((1, 1))\n",
        "\n",
        "        fake_output = discriminator(fake_data)\n",
        "        d_fake_loss = criterion(fake_output, fake_labels)\n",
        "\n",
        "        d_loss = d_real_loss + d_fake_loss\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        generator.zero_grad()\n",
        "        noise = torch.randn(1, 100)\n",
        "        fake_data = generator(noise)\n",
        "        fake_labels = torch.ones((1, 1))\n",
        "\n",
        "        output = discriminator(fake_data)\n",
        "        g_loss = criterion(output, fake_labels)\n",
        "\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Generator Loss: \" + \\\n",
        "              f\"{g_loss.item():.4f}, Discriminator Loss: {d_loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "6FJeEUYuD60o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan_small_subset(data_subset, generator, discriminator, num_epochs, batch_size=32):\n",
        "    \"\"\"\n",
        "        Trains a GAN using a subset of a dataset.\n",
        "\n",
        "        Args:\n",
        "            data_subset (numpy.ndarray): Subset of data points for training.\n",
        "            generator (Generator): Generator network.\n",
        "            discriminator (Discriminator): Discriminator network.\n",
        "            num_epochs (int): Number of epochs for training.\n",
        "            batch_size (int, optional): Batch size for training. Defaults to 32.\n",
        "    \"\"\"\n",
        "    criterion = nn.BCELoss()\n",
        "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
        "    g_optimizer = optim.Adam(generator.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i in range(0, len(data_subset), batch_size):\n",
        "            # Train Discriminator\n",
        "            discriminator.zero_grad()\n",
        "\n",
        "            real_data = torch.tensor(data_subset[i:i+batch_size]).float()\n",
        "            real_labels = torch.ones((len(real_data), 1))\n",
        "\n",
        "            real_output = discriminator(real_data)\n",
        "            d_real_loss = criterion(real_output, real_labels)\n",
        "\n",
        "            noise = torch.randn(len(real_data), 100)\n",
        "            fake_data = generator(noise)\n",
        "            fake_labels = torch.zeros((len(fake_data), 1))\n",
        "\n",
        "            fake_output = discriminator(fake_data)\n",
        "            d_fake_loss = criterion(fake_output, fake_labels)\n",
        "\n",
        "            d_loss = d_real_loss + d_fake_loss\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # Train Generator\n",
        "            generator.zero_grad()\n",
        "            noise = torch.randn(len(real_data), 100)\n",
        "            fake_data = generator(noise)\n",
        "            fake_labels = torch.ones((len(fake_data), 1))\n",
        "\n",
        "            output = discriminator(fake_data)\n",
        "            g_loss = criterion(output, fake_labels)\n",
        "\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Generator Loss: \" + \\\n",
        "              f\"{g_loss.item():.4f}, Discriminator Loss: {d_loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "d0MD9raZFy9X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def generate_synthetic_data(generator, num_samples):\n",
        "    \"\"\"\n",
        "        Generates synthetic data using the trained Generator\n",
        "\n",
        "        Args:\n",
        "        generator (Generator): Trained Generator network.\n",
        "        num_samples (int): Number of synthetic samples to generate.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Synthetic data.\n",
        "            numpy.ndarray: Synthetic labels.\n",
        "    \"\"\"\n",
        "    noise = torch.randn(num_samples, 100)\n",
        "    synthetic_data = generator(noise).detach().numpy()\n",
        "    synthetic_labels = np.random.randint(0, 2, size=(num_samples,))\n",
        "    return synthetic_data, synthetic_labels"
      ],
      "metadata": {
        "id": "jAGeZKwSAJSS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_shadow_model(X_train, y_train, X_test, y_test, num_epochs):\n",
        "    \"\"\"\n",
        "        Trains the shadow model.\n",
        "\n",
        "        Args:\n",
        "            X_train (torch.Tensor): Training data.\n",
        "            y_train (torch.Tensor): Training labels.\n",
        "            X_test (torch.Tensor): Test data.\n",
        "            y_test (torch.Tensor): Test labels.\n",
        "\n",
        "        Returns:\n",
        "            ShadowModel: Trained shadow model.\n",
        "    \"\"\"\n",
        "    shadow_model = ShadowModel()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(shadow_model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = shadow_model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        shadow_model.eval()\n",
        "        outputs = shadow_model(X_test)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = accuracy_score(predicted.numpy(), y_test.numpy())\n",
        "        print(f\"Accuracy of the shadow model: {accuracy}\")\n",
        "\n",
        "    return shadow_model"
      ],
      "metadata": {
        "id": "ksKbKfArAQiv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def membership_inference_attack(original_data, original_labels, shadow_model):\n",
        "    \"\"\"\n",
        "        Performs membership inference attack on the entire original dataset to\n",
        "        measure the shadow model's accuracy.\n",
        "\n",
        "        Args:\n",
        "            original_data (numpy.ndarray): Original data.\n",
        "            original_labels (numpy.ndarray): Original labels.\n",
        "            shadow_model (ShadowModel): Shadow model used for inference attack.\n",
        "    \"\"\"\n",
        "    correct_predictions = 0\n",
        "    total_samples = len(original_data)\n",
        "\n",
        "    for i in range(total_samples):\n",
        "        data_point = original_data[i]\n",
        "        label = original_labels[i]\n",
        "\n",
        "        # Use the shadow model to predict whether\n",
        "        # the data point was used in training\n",
        "        data_point_tensor = torch.tensor(data_point).float().unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            shadow_model.eval()\n",
        "            output = shadow_model(data_point_tensor)\n",
        "            predicted_label = torch.argmax(output).item()\n",
        "\n",
        "        if label == predicted_label:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    attack_accuracy = correct_predictions / total_samples\n",
        "    print(\"Membership Inference Attack Accuracy on Original Dataset: \" + \\\n",
        "          f\"{attack_accuracy}\")"
      ],
      "metadata": {
        "id": "iKyW767wAV9J"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Select a single row from the original dataset for GAN training\n",
        "selected_row_idx = np.random.randint(0, len(X))\n",
        "selected_data_point = X[selected_row_idx]\n",
        "\n",
        "# Train the GAN with the selected data point\n",
        "generator_single_data_point = Generator(100, 30)\n",
        "discriminator_single_data_point = Discriminator(30)\n",
        "train_gan_single_data_point(selected_data_point, generator_single_data_point,\n",
        "                            discriminator_single_data_point, num_epochs)\n",
        "\n",
        "# Train the GAN with a small subset of the original dataset\n",
        "selected_subset_indices = np.random.choice(len(X), size=10, replace=False)\n",
        "selected_subset = X[selected_subset_indices]\n",
        "generator_subset = Generator(100, 30)\n",
        "discriminator_subset = Discriminator(30)\n",
        "train_gan_small_subset(selected_subset, generator_subset, discriminator_subset,\n",
        "                       num_epochs)\n",
        "\n",
        "# Generate synthetic data using the Generator trained with single data point\n",
        "synthetic_data_single_data_point, synthetic_labels_single_data_point = \\\n",
        "    generate_synthetic_data(generator_single_data_point, len(X))\n",
        "\n",
        "# Generate synthetic data using the Generator trained with the subset of the original data\n",
        "synthetic_data_subset, synthetic_labels_subset = generate_synthetic_data(\n",
        "    generator_subset, len(X))\n",
        "\n",
        "# Split the data into train and test sets for target model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)#, random_state=42)\n",
        "\n",
        "# Train the target model with entire training dataset\n",
        "target_model = train_target_model(torch.tensor(X_train).float(),\n",
        "                                  torch.tensor(y_train),\n",
        "                                  torch.tensor(X_test).float(),\n",
        "                                  torch.tensor(y_test), num_epochs)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"Training shadow model with synthetic dataset created from a single data point...\")\n",
        "shadow_model_single_data_point = train_shadow_model(\n",
        "    torch.tensor(synthetic_data_single_data_point).float(),\n",
        "    torch.tensor(synthetic_labels_single_data_point),\n",
        "    torch.tensor(X_test).float(), torch.tensor(y_test), num_epochs)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"Training shadow model with synthetic dataset created from a subset of the original dataset...\")\n",
        "shadow_model_subset = train_shadow_model(\n",
        "    torch.tensor(synthetic_data_subset).float(),\n",
        "    torch.tensor(synthetic_labels_subset), torch.tensor(X_test).float(),\n",
        "    torch.tensor(y_test), num_epochs)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"Running MIA using the shadow model trained on synthetic data created from a single data point...\")\n",
        "membership_inference_attack(X, y, shadow_model_single_data_point)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"Running MIA using the shadow model trained on synthetic data created from a subset of the original dataset...\")\n",
        "membership_inference_attack(X, y, shadow_model_subset)"
      ],
      "metadata": {
        "id": "zjIiwQWKAYGv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}